{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybe8CY07zp7D"
      },
      "source": [
        "## Scenario - Student in Campus\n",
        "\n",
        "* Rahul is 2022 Entry Electrical Engineering student who just arrived the campus. However he is not currently familiar with the campus. He is very bright student and also a good sports person. \n",
        "\n",
        "As his coach is very strict and punctual and to avoid the punishment of arriving late in the playground he need to find the policy to reach the playground from any point of the campus as fast as possible. \n",
        "\n",
        "As a junior he reach out to us for to help him find that policy. Since we already know reinforcement learning we made a small model of IITD campus to train our agent to find that policy. \n",
        "* We will use Q-learning to accomplish this task!\n",
        "\n",
        "#### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rfdhGGMsw1H7"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq-QPfDnx4Fo"
      },
      "source": [
        "## Define the Environment\n",
        "The environment consists of **states**, **actions**, and **rewards**. States and actions are inputs for the Q-learning AI agent, while the possible actions are the AI agent's outputs.\n",
        "#### States\n",
        "The states in the environment are all of the possible locations within the campus. Some of these locations are for storing items (**black squares**), while other locations are aisles that the robot can use to travel throughout the warehouse (**white squares**). The **green square** indicates the item packaging and shipping area.\n",
        "\n",
        "The black and green squares are **terminal states**!\n",
        "\n",
        "![warehouse map](https://www.danielsoper.com/teaching/img/08-warehouse-map.png)\n",
        "\n",
        "The AI agent's goal is to learn the shortest path between the item packaging area and all of the other locations in the warehouse where the robot is allowed to travel.\n",
        "\n",
        "As shown in the image above, there are 121 possible states (locations) within the warehouse. These states are arranged in a grid containing 11 rows and 11 columns. Each location can hence be identified by its row and column index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9AdpFVfy6ya9"
      },
      "outputs": [],
      "source": [
        "#define the shape of the environment (i.e., its states)\n",
        "environment_rows = 14\n",
        "environment_columns = 14\n",
        "\n",
        "#Create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a) \n",
        "#The array contains 11 rows and 11 columns (to match the shape of the environment), as well as a third \"action\" dimension.\n",
        "#The \"action\" dimension consists of 4 layers that will allow us to keep track of the Q-values for each possible action in\n",
        "#each state (see next cell for a description of possible actions). \n",
        "#The value of each (state, action) pair is initialized to 0.\n",
        "guddy = np.zeros((environment_rows, environment_columns))\n",
        "q_values = np.zeros((environment_rows, environment_columns, 4))\n",
        "q_values_tutor = np.zeros((environment_rows, environment_columns, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07gGSNz07xtP"
      },
      "source": [
        "#### Actions\n",
        "The actions that are available to the AI agent are to move the robot in one of four directions:\n",
        "* Up\n",
        "* Right\n",
        "* Down\n",
        "* Left\n",
        "\n",
        "Obviously, the AI agent must learn to avoid driving into the item storage locations (e.g., shelves)!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z43QX_t080q3"
      },
      "outputs": [],
      "source": [
        "#define actions\n",
        "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
        "actions = ['up', 'right', 'down', 'left']\n",
        "global cnt\n",
        "cnt = 0\n",
        "global tt\n",
        "tt = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X25vn4VKw2as"
      },
      "source": [
        "#### Rewards\n",
        "The last component of the environment that we need to define are the **rewards**. \n",
        "\n",
        "To help the AI agent learn, each state (location) in the warehouse is assigned a reward value.\n",
        "\n",
        "The agent may begin at any white square, but its goal is always the same: ***to maximize its total rewards***!\n",
        "\n",
        "Negative rewards (i.e., **punishments**) are used for all states except the goal.\n",
        "* This encourages the AI to identify the *shortest path* to the goal by *minimizing its punishments*!\n",
        "\n",
        "![IIT map](iit_map.jpeg)\n",
        "\n",
        "To maximize its cumulative rewards (by minimizing its cumulative punishments), the AI agent will need find the shortest paths between the item packaging area (green square) and all of the other locations in the warehouse where the robot is allowed to travel (white squares). The agent will also need to learn to avoid crashing into any of the item storage locations (black squares)!\n",
        "\n",
        "![Matrix Form](matrix_form.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GIJu7XsLXw62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1000. -1000. -1000. -1000. -1000. -1000. -1000. -1000.  1000. -1000.\n",
            " -1000. -1000. -1000. -1000.]\n",
            "[-1000. -1000. -1000. -1000. -1000.    -1.    -1.    -1.    -1.    -1.\n",
            "    -1.    -1.    -1.    -1.]\n",
            "[-1000. -1000. -1000. -1000. -1000.    -1. -1000. -1000. -1000.    -1.\n",
            " -1000. -1000. -1000.    -1.]\n",
            "[-1000. -1000. -1000. -1000. -1000.    -1. -1000. -1000. -1000.    -1.\n",
            " -1000. -1000. -1000.    -1.]\n",
            "[-1000. -1000. -1000. -1000. -1000.    -1.    -1.    -1.    -1.    -1.\n",
            "    -1.    -1.    -1. -1000.]\n",
            "[-1000. -1000. -1000. -1000. -1000.    -1. -1000. -1000. -1000.    -1.\n",
            " -1000. -1000. -1000. -1000.]\n",
            "[-1000. -1000.    -1. -1000. -1000.    -1. -1000. -1000. -1000.    -1.\n",
            " -1000. -1000. -1000. -1000.]\n",
            "[-1000. -1000.    -1.    -1.    -1.    -1. -1000. -1000. -1000.    -1.\n",
            " -1000. -1000. -1000. -1000.]\n",
            "[-1000. -1000. -1000. -1000. -1000.    -1.    -1.    -1.    -1.    -1.\n",
            "    -1.    -1. -1000. -1000.]\n",
            "[-1000. -1000. -1000. -1000. -1000.    -1. -1000. -1000. -1000.    -1.\n",
            " -1000. -1000. -1000. -1000.]\n",
            "[-1000.    -1.    -1.    -1.    -1.    -1. -1000. -1000. -1000.    -1.\n",
            " -1000. -1000. -1000. -1000.]\n",
            "[-1000. -1000. -1000. -1000. -1000.    -1.    -1.    -1.    -1.    -1.\n",
            " -1000. -1000. -1000. -1000.]\n",
            "[   -1.    -1.    -1.    -1.    -1.    -1. -1000. -1000. -1000. -1000.\n",
            " -1000. -1000. -1000. -1000.]\n",
            "[-1000. -1000. -1000. -1000. -1000.    -1. -1000. -1000. -1000.    -1.\n",
            " -1000. -1000. -1000. -1000.]\n"
          ]
        }
      ],
      "source": [
        "#Create a 2D numpy array to hold the rewards for each state. \n",
        "#The array contains 11 rows and 11 columns (to match the shape of the environment), and each value is initialized to -100.\n",
        "rewards = np.full((environment_rows, environment_columns), -1000.)\n",
        "rewards[0, 8] = 1000. #set the reward for the packaging area (i.e., the goal) to 100\n",
        "\n",
        "#define aisle locations (i.e., white squares) for rows 1 through 9\n",
        "aisles = {} #store locations in a dictionary\n",
        "aisles[1] = [i for i in range(5, 14)]\n",
        "aisles[2] = [5, 9, 13]\n",
        "aisles[3] = [5, 9, 13]\n",
        "aisles[4] = [i for i in range(5, 13)]\n",
        "aisles[5] = [5, 9]\n",
        "aisles[6] = [2, 5, 9]\n",
        "aisles[7] = [i for i in range(2,6)]\n",
        "aisles[7].append(9)\n",
        "aisles[8] = [i for i in range(5, 12)]\n",
        "aisles[9] = [5, 9]\n",
        "aisles[10] = [i for i in range(1,6)]\n",
        "aisles[10].append(9)\n",
        "aisles[11] = [i for i in range(5,10)]\n",
        "aisles[12] = [i for i in range(0,6)]\n",
        "aisles[13] = [5, 9]\n",
        "\n",
        "x_final  = 0\n",
        "y_final = 8\n",
        "\n",
        "MOST_NEG = -1*1000000\n",
        "N = 6000\n",
        "\n",
        "\n",
        "indeces = [[3,9],[5,0],[9,5]]\n",
        "\n",
        "ideal_reward = [4,7,9] #dummy\n",
        "\n",
        "#set the rewards for all aisle locations (i.e., white squares)\n",
        "for row_index in range(1, 14):\n",
        "  for column_index in aisles[row_index]:\n",
        "    rewards[row_index, column_index] = -1.\n",
        "  \n",
        "#print rewards matrix\n",
        "for row in rewards:\n",
        "  print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFEor01iCCin"
      },
      "source": [
        "## Train the Model\n",
        "Our next task is for our AI agent to learn about its environment by implementing a Q-learning model. The learning process will follow these steps:\n",
        "1. Choose a random, non-terminal state (white square) for the agent to begin this new episode.\n",
        "2. Choose an action (move *up*, *right*, *down*, or *left*) for the current state. Actions will be chosen using an *epsilon greedy algorithm*. This algorithm will usually choose the most promising action for the AI agent, but it will occasionally choose a less promising option in order to encourage the agent to explore the environment.\n",
        "3. Perform the chosen action, and transition to the next state (i.e., move to the next location).\n",
        "4. Receive the reward for moving to the new state, and calculate the temporal difference.\n",
        "5. Update the Q-value for the previous state and action pair.\n",
        "6. If the new (current) state is a terminal state, go to #1. Else, go to #2.\n",
        "\n",
        "This entire process will be repeated across 1000 episodes. This will provide the AI agent sufficient opportunity to learn the shortest paths between the item packaging area and all other locations in the warehouse where the robot is allowed to travel, while simultaneously avoiding crashing into any of the item storage locations!\n",
        "\n",
        "#### Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DnCfO5tVG0LJ"
      },
      "outputs": [],
      "source": [
        "#define a function that determines if the specified location is a terminal state\n",
        "def is_terminal_state(current_row_index, current_column_index):\n",
        "  #if the reward for this location is -1, then it is not a terminal state (i.e., it is a 'white square')\n",
        "  if rewards[current_row_index, current_column_index] == -1.:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "#define a function that will choose a random, non-terminal starting location\n",
        "def get_starting_location():\n",
        "  #get a random row and column index\n",
        "  current_row_index = np.random.randint(environment_rows)\n",
        "  current_column_index = np.random.randint(environment_columns)\n",
        "  #continue choosing random row and column indexes until a non-terminal state is identified\n",
        "  #(i.e., until the chosen state is a 'white square').\n",
        "  while is_terminal_state(current_row_index, current_column_index):\n",
        "    current_row_index = np.random.randint(environment_rows)\n",
        "    current_column_index = np.random.randint(environment_columns)\n",
        "  return current_row_index, current_column_index\n",
        "\n",
        "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
        "\n",
        "def get_next_action(current_row_index, current_column_index, epsilon):\n",
        "  #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
        "  #then choose the most promising value from the Q-table for this state.\n",
        "  if np.random.random() < epsilon:\n",
        "    return np.argmax(q_values[current_row_index, current_column_index])\n",
        "  else: #choose a random action\n",
        "    return np.random.randint(4)\n",
        "\n",
        "\n",
        "#define a function that will get the next location based on the chosen action\n",
        "def get_next_location(current_row_index, current_column_index, action_index):\n",
        "  new_row_index = current_row_index\n",
        "  new_column_index = current_column_index\n",
        "  if actions[action_index] == 'up' and current_row_index > 0:\n",
        "    new_row_index -= 1\n",
        "  elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
        "    new_column_index += 1\n",
        "  elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
        "    new_row_index += 1\n",
        "  elif actions[action_index] == 'left' and current_column_index > 0:\n",
        "    new_column_index -= 1\n",
        "  return new_row_index, new_column_index\n",
        "\n",
        "#Define a function that will get the shortest path between any location within the warehouse that \n",
        "#the robot is allowed to travel and the item packaging location.\n",
        "\n",
        "def get_reward(start_row_index, start_column_index):\n",
        "      #return immediately if this is an invalid starting location\n",
        "  if is_terminal_state(start_row_index, start_column_index):\n",
        "    return MOST_NEG\n",
        "  else: #if this is a 'legal' starting location\n",
        "    current_row_index, current_column_index = start_row_index, start_column_index\n",
        "  #shortest_path = []\n",
        "    reward = 0\n",
        "  if current_row_index == x_final and current_column_index == y_final:\n",
        "    reward  = reward + 100\n",
        "    return reward\n",
        "  else:\n",
        "    reward = reward -1\n",
        "  #shortest_path.append([current_row_index, current_column_index])\n",
        "  #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
        "  while not is_terminal_state(current_row_index, current_column_index):\n",
        "    #get the best action to take\n",
        "    action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
        "    #move to the next location on the path, and add the new location to the list\n",
        "    current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
        "    reward = reward -1\n",
        "    #shortest_path.append([current_row_index, current_column_index])\n",
        "  if current_row_index == x_final and current_column_index == y_final:\n",
        "    reward = reward + 100\n",
        "  else:\n",
        "    reward = reward-100\n",
        "  return reward\n",
        "\n",
        "def get_shortest_path(start_row_index, start_column_index):\n",
        "  #return immediately if this is an invalid starting location\n",
        "  if is_terminal_state(start_row_index, start_column_index):\n",
        "    return []\n",
        "  else: #if this is a 'legal' starting location\n",
        "    current_row_index, current_column_index = start_row_index, start_column_index\n",
        "    shortest_path = []\n",
        "    shortest_path.append([current_row_index, current_column_index])\n",
        "    #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
        "    while not is_terminal_state(current_row_index, current_column_index):\n",
        "      #get the best action to take\n",
        "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
        "      #move to the next location on the path, and add the new location to the list\n",
        "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
        "      shortest_path.append([current_row_index, current_column_index])\n",
        "    return shortest_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function for control tutor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(current_row_index, current_column_index):\n",
        "  x = current_row_index\n",
        "  y = current_column_index\n",
        "  arr = []\n",
        "  # if x>x_final:\n",
        "  #   if (not is_terminal_state(x-1,y)) or rewards[x-1][y] == 100:\n",
        "  #     #return 0\n",
        "  #     arr.append(0)\n",
        "  # if y<y_final:\n",
        "  #   if (not is_terminal_state(x,y+1)) or rewards[x][y+1] == 100:\n",
        "  #     arr.append(1)\n",
        "  # if y>y_final and y<14:\n",
        "  #   if (not is_terminal_state(x,y-1)) or rewards[x][y-1] == 100:\n",
        "  #     arr.append(3)\n",
        "  #return 2\n",
        "  if x>0:\n",
        "    if (not is_terminal_state(x-1,y)) or rewards[x-1][y] == 100:\n",
        "      #return 0\n",
        "      arr.append(0)\n",
        "  if y<13:\n",
        "    if (not is_terminal_state(x,y+1)) or rewards[x][y+1] == 100:\n",
        "      arr.append(1)\n",
        "  if y>0:\n",
        "    if (not is_terminal_state(x,y-1)) or rewards[x][y-1] == 100:\n",
        "      arr.append(3)\n",
        "  if y+1<14:\n",
        "    if (not is_terminal_state(x,y+1)) or rewards[x][y+1] == 100:\n",
        "      arr.append(3)\n",
        "\n",
        "  if len(arr) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return arr[np.random.randint(len(arr))]\n",
        "\n",
        "def get_next_action_by_tutor(current_row_index, current_column_index, epsilon):\n",
        "      #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
        "  #then choose the most promising value from the Q-table for this state.\n",
        "  # k = np.argmax(q_values_tutor[current_row_index, current_column_index])\n",
        "  k = np.random.random()\n",
        "  i = current_row_index\n",
        "  j = current_column_index\n",
        "  if k<0.25:\n",
        "      if np.random.random() < epsilon:\n",
        "          return get_next_action(current_row_index, current_column_index, 1.)\n",
        "      else:\n",
        "        return np.random.randint(4)\n",
        "  else:\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.argmax(q_values_tutor[current_row_index, current_column_index])\n",
        "    else: #choose a random action\n",
        "        return np.random.randint(4)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def averageReward(arr):\n",
        "    if len(arr)==0:\n",
        "        return 0\n",
        "    else:\n",
        "        return sum(arr) / len(arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjl9niKEqONs"
      },
      "source": [
        "## Train the AI Agent using Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
        "discount_factor = 0.9 #discount factor for future rewards\n",
        "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
        "\n",
        "def train():\n",
        "#run through 1000 training episodes\n",
        "  episode_reward = []\n",
        "  average_reward = []\n",
        "  for episode in range(N):\n",
        "    #get the starting location for this episode\n",
        "    row_index, column_index = get_starting_location()\n",
        "\n",
        "    cur_reward = 0 #cur reward of episode\n",
        "    #continue taking actions (i.e., moving) until we reach a terminal state\n",
        "    #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
        "    while not is_terminal_state(row_index, column_index):\n",
        "      #choose which action to take (i.e., where to move next)\n",
        "      action_index = get_next_action(row_index, column_index, epsilon)\n",
        "\n",
        "      #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
        "      old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
        "      row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
        "      \n",
        "      #receive the reward for moving to the new state, and calculate the temporal difference\n",
        "      reward = rewards[row_index, column_index]\n",
        "      cur_reward+=reward #add reward obtained to current reward\n",
        "\n",
        "      old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
        "      temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
        "\n",
        "      #update the Q-value for the previous state and action pair\n",
        "      new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
        "      q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
        "\n",
        "    episode_reward.append(cur_reward) #add reward to the episode\n",
        "    average_reward.append(averageReward(episode_reward))\n",
        "  return episode_reward,average_reward\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the AI Agent using Control Tutored Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3N5BB0m0JHIn"
      },
      "outputs": [],
      "source": [
        "#define training parameters\n",
        "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
        "discount_factor = 0.9 #discount factor for future rewards\n",
        "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
        "\n",
        "def train_by_tutor():\n",
        "#run through 1000 training episodes\n",
        "  episode_reward = []\n",
        "  average_reward = []\n",
        "  for episode in range(N):\n",
        "    #get the starting location for this episode\n",
        "    row_index, column_index = get_starting_location()\n",
        "    cur_reward = 0\n",
        "    #continue taking actions (i.e., moving) until we reach a terminal state\n",
        "    #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
        "    while not is_terminal_state(row_index, column_index) :#and cur_reward>-200:\n",
        "      #choose which action to take (i.e., where to move next)\n",
        "      action_index = get_next_action_by_tutor(row_index, column_index, epsilon)\n",
        "\n",
        "      #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
        "      old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
        "      row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
        "      \n",
        "      #receive the reward for moving to the new state, and calculate the temporal difference\n",
        "      reward = rewards[row_index, column_index]\n",
        "      \n",
        "      cur_reward+=reward #add reward obtained to current reward\n",
        "      \n",
        "      old_q_value = q_values_tutor[old_row_index, old_column_index, action_index]\n",
        "      temporal_difference = reward + (discount_factor * np.max(q_values_tutor[row_index, column_index])) - old_q_value\n",
        "\n",
        "      #update the Q-value for the previous state and action pair\n",
        "      new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
        "      q_values_tutor[old_row_index, old_column_index, action_index] = new_q_value\n",
        "    \n",
        "    episode_reward.append(cur_reward) #add reward to the episode\n",
        "    average_reward.append(averageReward(episode_reward))\n",
        "  return episode_reward,average_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JqQfjYdfyBh"
      },
      "source": [
        "## Get Shortest Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#display a few shortest paths\n",
        "classic_reward = train()\n",
        "train()\n",
        "control_tutored_reward = train_by_tutor()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "F1YO3mj_oS2J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Classic Reward-294.5643333333333\n",
            "Average Control Tutored Reward-296.186\n",
            "Maximum Classic Reward-1000.0\n",
            "Maximum Control Tutored Reward-1000.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "cur_reward_classic = classic_reward[0]\n",
        "cur_reward_control_tutor = control_tutored_reward[0]\n",
        "\n",
        "average_reward_classic = classic_reward[1]\n",
        "average_reward_control_tutor = control_tutored_reward[1]\n",
        "\n",
        "print(\"Average Classic Reward-\"+str(average_reward_classic[-1]))\n",
        "print(\"Average Control Tutored Reward-\"+str(average_reward_control_tutor[-1]))\n",
        "\n",
        "print(\"Maximum Classic Reward-\"+str(max(cur_reward_classic)))\n",
        "print(\"Maximum Control Tutored Reward-\"+str(max(cur_reward_control_tutor)))\n",
        "\n",
        "# print(get_shortest_path(3, 9)) #starting at row 3, column 9\n",
        "# print(get_shortest_path(5, 0)) #starting at row 5, column 0\n",
        "# print(get_shortest_path(9, 5)) #starting at row 9, column 5\n",
        "xpoints = [x for x in range(N)]\n",
        "\n",
        "\n",
        "plt.plot(xpoints, average_reward_classic,label = \"Classic\")\n",
        "plt.plot(xpoints, average_reward_control_tutor,label = \"Control Tutored\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "q_values.fill(0)\n",
        "q_values_tutor.fill(0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compareBySession(S):\n",
        "    avg_rew_cl=[]\n",
        "    avg_rew_ct=[]\n",
        "\n",
        "    cur_rew_cl=[]\n",
        "    cur_rew_ct=[]\n",
        "    for i in range(S):\n",
        "        q_values.fill(0)\n",
        "        q_values_tutor.fill(0)\n",
        "        \n",
        "        classic_reward = train()\n",
        "        control_tutored_reward = train_by_tutor()\n",
        "\n",
        "        cur_reward_classic = classic_reward[0]\n",
        "        cur_reward_control_tutor = control_tutored_reward[0]\n",
        "\n",
        "        average_reward_classic = classic_reward[1]\n",
        "        average_reward_control_tutor = control_tutored_reward[1]\n",
        "\n",
        "        cur_rew_cl.append(average_reward_classic[-1])\n",
        "        cur_rew_ct.append(average_reward_control_tutor[-1])\n",
        "\n",
        "        avg_rew_cl.append(averageReward(cur_rew_cl))\n",
        "        avg_rew_ct.append(averageReward(cur_rew_ct))\n",
        "        \n",
        "    return avg_rew_cl, avg_rew_ct\n",
        "\n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# S=20\n",
        "# xpoints = [x for x in range(S)]\n",
        "\n",
        "# comparisonResult = compareBySession(S)\n",
        "\n",
        "# average_reward_classic = comparisonResult[0]\n",
        "# average_reward_control_tutor = comparisonResult[1]\n",
        "\n",
        "# print(\"Average Classic Reward- \"+str(average_reward_classic[-1]))\n",
        "# print(\"Average Control Tutored Reward- \"+str(average_reward_control_tutor[-1]))\n",
        "\n",
        "# plt.plot(xpoints, average_reward_classic,label = \"Classic\")\n",
        "# plt.plot(xpoints, average_reward_control_tutor,label = \"Control Tutored\")\n",
        "# plt.legend(loc=\"upper left\")\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWx7BsJxqrDv"
      },
      "source": [
        "#### Finally...\n",
        "It's great that our robot can automatically take the shortest path from any 'legal' location in the warehouse to the item packaging area. **But what about the opposite scenario?**\n",
        "\n",
        "Put differently, our robot can currently deliver an item from anywhere in the warehouse ***to*** the packaging area, but after it delivers the item, it will need to travel ***from*** the packaging area to another location in the warehouse to pick up the next item!\n",
        "\n",
        "Don't worry -- this problem is easily solved simply by ***reversing the order of the shortest path***.\n",
        "\n",
        "Run the code cell below to see an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fKun8LInsas9"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19536/625268907.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_shortest_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19536/1042449772.py\u001b[0m in \u001b[0;36mget_shortest_path\u001b[1;34m(start_row_index, start_column_index)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_terminal_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_row_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_column_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m       \u001b[1;31m#get the best action to take\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m       \u001b[0maction_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_next_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_row_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_column_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m       \u001b[1;31m#move to the next location on the path, and add the new location to the list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m       \u001b[0mcurrent_row_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_column_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_next_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_row_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_column_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19536/1042449772.py\u001b[0m in \u001b[0;36mget_next_action\u001b[1;34m(current_row_index, current_column_index, epsilon)\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[1;31m#then choose the most promising value from the Q-table for this state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_row_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_column_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#choose a random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;32mc:\\python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \"\"\"\n\u001b[1;32m-> 1195\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#display an example of reversed shortest path\n",
        "train()\n",
        "def optimum_action(i, j, x, y):\n",
        "    #print(i,j,x,y)\n",
        "    if i>x:\n",
        "        return 0\n",
        "    if i<x:\n",
        "        return 2\n",
        "    if j<y:\n",
        "        return 1\n",
        "    return 3\n",
        "\n",
        "for i in range(0, environment_columns):\n",
        "    for j in range(0, environment_columns):\n",
        "        path = get_shortest_path(i,j)\n",
        "        if len(path)>1:\n",
        "            x = path[1][0]\n",
        "            y = path[1][1]\n",
        "            #print(i,j,x,y)\n",
        "            guddy[i][j] = optimum_action(i,j,x,y)\n",
        "\n",
        "print(optimum_action(0,2,4,5))\n",
        "print(guddy)\n",
        "path = get_shortest_path(10, 1) #go to row 5, column 2\n",
        "print(path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Notebook for Topic 08 Video - Q-Learning - A Complete Example in Python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "996f2b6e98019c29df53ccff772a8e3fe7cdecb0ba4b3f6ddb9be804d297dd3d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
